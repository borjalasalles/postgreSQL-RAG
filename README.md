# PostgreSQL RAG Solution with Hybrid Search & Open-Source Intelligence

A comprehensive Retrieval-Augmented Generation (RAG) system featuring hybrid search capabilities, built with PostgreSQL, pgvectorscale, and Anthropic Claude. This project demonstrates advanced information retrieval techniques combining semantic understanding, keyword matching, and intelligent reranking‚Äîall while maintaining data privacy through open-source embedding models.

<img width="1613" height="1043" alt="image" src="https://github.com/user-attachments/assets/a8630c15-8955-4869-bd64-6adfa707a81d" />


## üéØ Project Overview

This implementation showcases an advanced RAG pipeline featuring **hybrid search capabilities** that combine the robustness of PostgreSQL with cutting-edge information retrieval techniques. The system integrates three complementary search methodologies: semantic vector search, PostgreSQL full-text search, and intelligent reranking through open-source cross-encoders.

Built entirely with open-source technologies (except for the final LLM synthesis), this solution demonstrates how to achieve enterprise-grade performance while maintaining complete data privacy and cost efficiency. The hybrid approach significantly outperforms single-method systems by leveraging both conceptual understanding and exact keyword matching.

### **Live Performance Metrics**
Based on real implementation benchmarks with 1,000 CNN news articles:
- **Semantic Search**: 4-48ms response time
- **Keyword Search**: 18-19ms response time  
- **Cross-Encoder Reranking**: ~600ms for improved relevance
- **End-to-End Query Processing**: Sub-second response times

## üîç Advanced Hybrid Search Architecture

This system implements a sophisticated three-tier search approach that addresses the limitations of single-method retrieval systems:

### **Semantic Vector Search**
Utilizes sentence-transformers models (`all-MiniLM-L6-v2`) to understand conceptual similarity between queries and documents. This approach excels at finding relevant content even when exact keywords don't match, enabling natural language understanding at the document level.

### **PostgreSQL Full-Text Search**
Leverages PostgreSQL's mature full-text search capabilities with GIN indexing for exact keyword matching. This method ensures that specific terms, names, or phrases are captured with precision, complementing the broader conceptual search.

### **Cross-Encoder Reranking**
Employs open-source cross-encoder models (`cross-encoder/ms-marco-MiniLM-L-6-v2`) to intelligently rerank combined results. Unlike simple score combination, cross-encoders evaluate query-document pairs directly, providing superior relevance assessment.

This hybrid methodology produces significantly more accurate results than any single approach, combining the breadth of semantic understanding with the precision of keyword matching and the intelligence of learned relevance ranking.

### Embedding Strategy: Open-Source Models
Rather than relying on external APIs for embeddings, this implementation uses sentence-transformers models that run locally:

**Selected model**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)
- Extremely fast inference (14-15ms per embedding)
- Compact size (~23MB) for easy deployment
- Excellent performance for most use cases
- No external API dependencies or costs

**Alternative models** are easily configurable through the settings system for different performance/quality trade-offs.

### LLM Integration: Anthropic Claude
The system integrates with Anthropic Claude through the instructor library, providing structured output generation and robust error handling. This choice offers superior reasoning capabilities compared to alternatives while maintaining clean, maintainable code architecture.

## üöÄ Technical Deep Dive: pgvectorscale vs pgvector

Understanding the difference between pgvector and pgvectorscale is crucial for appreciating this system's performance characteristics.

**pgvector** provides the foundation for vector operations in PostgreSQL, introducing the VECTOR data type and basic similarity search capabilities. While functional, it has limitations in indexing performance for large datasets.

**pgvectorscale** extends pgvector with advanced indexing algorithms, particularly the DiskANN-inspired index. This dramatically improves query performance through several innovations:

The **DiskANN index** is based on Microsoft Research's DiskANN algorithm, which creates a graph-based index optimized for disk storage. This allows for sub-linear search times even with millions of vectors.

**Statistical Binary Quantization** developed by Timescale researchers improves upon standard binary quantization techniques, reducing memory usage while maintaining search accuracy.

**Label-based filtered search** enables combining vector similarity search with metadata filtering, allowing for more precise and contextual results.

According to Timescale benchmarks, PostgreSQL with pgvectorscale achieves 28x lower p95 latency and 16x higher query throughput compared to dedicated vector databases like Pinecone, while reducing costs by approximately 75% when self-hosted.

## üìã Prerequisites

Before setting up this project, ensure you have the following installed on your system:

- **Docker and Docker Compose** for containerized database management
- **Python 3.8+** with pip for package management
- **PostgreSQL client tools** (optional, for direct database interaction)
- **DBeaver Community** or similar GUI client for database visualization

## üöÄ Quick Start Guide

### Prerequisites
- Docker and Docker Compose
- Python 3.8+
- Anthropic Claude API key
- Internet connection (for initial model downloads)

### Step-by-Step Setup

1. **Clone and Setup Environment**
```bash
git clone <your-repository-url>
cd postgresql-rag-solution
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. **Configure API Key**
```bash
# Edit app/services/.env with your real Anthropic API key
# ANTHROPIC_API_KEY=your_actual_key_here
```

3. **Start Database**
```bash
docker-compose up -d
```

4. **Load and Vectorize Dataset**
```bash
python app/insert_vectors.py
# Downloads CNN/DailyMail articles and creates embeddings (~2-3 minutes)
```

5. **Create Search Indices**
```bash
python app/create_text_index.py
# Creates GIN index for keyword search
```

6. **Test the System**
```bash
python app/search.py
# Demonstrates semantic, keyword, and hybrid search with AI synthesis
```

## üõ†Ô∏è Detailed Installation & Configuration
```bash
git clone <your-repository-url>
cd postgresql-rag-solution
```

### Step 2: Database Setup with Docker
Create the docker-compose.yaml file for your PostgreSQL instance with pgvectorscale:

```yaml
version: '3.8'

services:
  timescaledb:
    image: timescale/timescaledb-ha:pg16
    container_name: timescaledb
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  timescaledb_data:
```

Launch the database container:
```bash
docker-compose up -d
```

### Step 3: Python Environment Setup
Create and activate a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

Install system dependencies for PostgreSQL connectivity:
```bash
sudo apt update
sudo apt install postgresql-server-dev-all libpq-dev
```

Install Python dependencies:
```bash
pip install -r requirements.txt
```

### Step 4: Environment Configuration
Create a `.env` file in your project root:

```env
# Anthropic API Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-haiku-20241022
ANTHROPIC_TEMPERATURE=0.2
ANTHROPIC_MAX_TOKENS=1000
ANTHROPIC_TIMEOUT=300

# Database Configuration
TIMESCALE_SERVICE_URL=postgres://postgres:password@localhost:5432/postgres
```

### Step 5: Initialize Database and Create Indices
```bash
# Insert sample data (CNN/DailyMail articles)
python app/insert_vectors.py

# Create GIN index for keyword search
python app/create_text_index.py
```

The system creates two specialized indices:
- **DiskANN Index**: For high-performance semantic vector search
- **GIN Index**: For PostgreSQL full-text keyword search

## üìä Core Components

### Vector Store Implementation
The `VectorStore` class serves as the primary interface for vector operations, wrapping the timescale-vector library with additional functionality:

```python
from app.vector_store import VectorStore

# Initialize the vector store
vs = VectorStore()

# Create necessary database tables
vs.create_tables()

# Generate and insert embeddings from your data
# (See insert_vectors.py for complete implementation)

# Create high-performance index for fast similarity search
vs.create_index()

# Perform similarity searches
results = vs.search("your query here", limit=5)
```

### Understanding Index Creation
The index creation step is where pgvectorscale truly differentiates itself from standard pgvector implementations. When you call `vs.create_index()`, the system creates a DiskANN index that transforms search performance:

**Without index**: Linear scan through all vectors (O(n) complexity)
**With DiskANN index**: Graph-based approximate nearest neighbor search (sub-linear complexity)

This difference becomes dramatic with larger datasets. For 10,000+ vectors, the index can provide 10-100x performance improvements in query response times.

### LLM Factory for Response Generation
The `LLMFactory` class provides a clean abstraction for language model interactions:

```python
from app.llm_factory import LLMFactory

# Initialize with Anthropic Claude
llm = LLMFactory("anthropic")

# Generate structured responses
response = llm.create_completion(
    response_model=YourPydanticModel,
    messages=[{"role": "user", "content": "Your query"}]
)
```

## üí° Usage Examples

### Basic Hybrid Search
```python
from database.vector_store import VectorStore

# Initialize the vector store
vs = VectorStore()

# Perform hybrid search combining semantic and keyword approaches
results = vs.hybrid_search(
    query="London news", 
    keyword_k=10,      # Top 10 from keyword search
    semantic_k=10,     # Top 10 from semantic search
    rerank=True,       # Apply cross-encoder reranking
    top_n=5           # Return top 5 after reranking
)

print(results[['content', 'search_type', 'relevance_score']])
```

### Individual Search Methods
```python
# Pure semantic search for conceptual understanding
semantic_results = vs.semantic_search("British politics", limit=5)

# Pure keyword search for exact term matching  
keyword_results = vs.keyword_search("London Olympics", limit=5)

# Hybrid without reranking (faster)
hybrid_results = vs.hybrid_search("royal family", rerank=False)
```

### Complete RAG Pipeline
```python
from services.synthesizer import Synthesizer

# Perform hybrid search and synthesize response
query = "What happened at the London Olympics opening ceremony?"
results = vs.hybrid_search(query, rerank=True, top_n=5)

# Generate intelligent response using Anthropic Claude
response = Synthesizer.generate_response(question=query, context=results)
print(response.answer)
print(f"Confidence: {response.enough_context}")
```

### Advanced Filtering with Metadata
```python
from timescale_vector import client
from datetime import datetime

# Search with time-based filtering
time_range = (datetime(2012, 1, 1), datetime(2012, 12, 31))
olympic_results = vs.semantic_search(
    "Olympic ceremony", 
    time_range=time_range,
    limit=5
)

# Complex predicates for advanced filtering
predicates = client.Predicates("category", "==", "Sports")
sports_results = vs.semantic_search("Olympics", predicates=predicates)
```

## üîß System Architecture Insights

### Timescale Vector Library Integration
This implementation uses the timescale-vector Python library as its foundation, but wraps it with additional functionality for better usability. The core methods in the `VectorStore` class are essentially clean interfaces to the underlying timescale-vector operations:

**Table creation** (`vs.create_tables()`) uses timescale-vector's default schema, which provides an optimal structure for most use cases while remaining flexible for customization.

**Index management** (`vs.create_index()` and `vs.drop_index()`) directly interfaces with the DiskANN index implementation, giving you access to state-of-the-art vector indexing without the complexity of manual configuration.

**Search operations** benefit from both the index performance and the library's optimized query planning, automatically selecting the best search strategy based on your query parameters.

### Performance Characteristics
The embedding generation pipeline achieves impressive throughput with the selected sentence-transformers model:
- Single embedding generation: 14-15 milliseconds
- Batch processing: Scales linearly with automatic batching
- Memory footprint: Minimal due to the compact model size

Database operations show excellent performance characteristics:
- Table creation: Instantaneous for most schemas
- Index creation: Scales with dataset size, typically completes in seconds for datasets under 100K vectors
- Search queries: Sub-millisecond response times with proper indexing

## üìà Performance Analysis & Real-World Benchmarks

The implementation demonstrates exceptional performance characteristics across all search modalities, tested with 1,000 CNN/DailyMail news articles:

**Embedding Generation Performance**: The sentence-transformers model achieves remarkable efficiency with embedding generation times consistently between 10-20 milliseconds per query. This local processing eliminates network latency and API costs while maintaining high-quality semantic representations.

**Database Query Performance**: PostgreSQL with pgvectorscale delivers outstanding search performance. Semantic searches complete in 4-48 milliseconds, while keyword searches using GIN indices execute in 18-19 milliseconds. The variation in semantic search times correlates with index warming and query complexity.

**Hybrid Search Intelligence**: The cross-encoder reranking process, while more computationally intensive at approximately 600 milliseconds, provides substantial relevance improvements. This investment in processing time yields significantly more accurate results by evaluating query-document relationships holistically rather than relying solely on individual vector similarities.

**System Scalability**: The dual-index architecture scales effectively with dataset size. The DiskANN index provides logarithmic search complexity for semantic queries, while PostgreSQL's mature GIN indexing ensures consistent keyword search performance even as document volumes increase substantially.

These benchmarks demonstrate that the hybrid approach achieves enterprise-grade performance while maintaining complete data privacy through local processing of embeddings and search operations.

## üìÑ Licensing & Commercial Rights

This project is released under the **Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0)**.

**What this means:**
- ‚úÖ **Free to use** for educational, research, and personal projects
- ‚úÖ **Free to modify** and adapt for non-commercial purposes
- ‚úÖ **Free to share** with proper attribution
- ‚ùå **Commercial use prohibited** without explicit permission

**For Commercial Usage**: If you wish to use this system in commercial applications, please contact the repository owner for licensing arrangements. Commercial licenses are available for enterprise deployment and integration.

**Why this license?**: This approach supports the open-source community while recognizing the substantial development effort invested in creating a production-ready hybrid search system. Educational and research use remains completely free, while commercial applications contribute to ongoing development.

## üîÆ Next Steps & Extensions

This foundation provides multiple pathways for enhancement and scaling:

**Hybrid Search Implementation**: Combine vector similarity with full-text search capabilities using PostgreSQL's built-in text search features.

**Advanced RAG Techniques**: Implement re-ranking algorithms, query expansion, and context window optimization for improved response quality.

**Production Deployment**: Add connection pooling, monitoring, backup strategies, and horizontal scaling configurations.

**Multi-Modal Support**: Extend the embedding pipeline to handle images, audio, or other data types using appropriate models.

## üõ°Ô∏è Production Considerations

When deploying this system in production environments, several additional considerations become important:

**Security**: Implement proper authentication, network isolation, and API key management. The current configuration uses basic authentication suitable for development.

**Monitoring**: Add logging, metrics collection, and alerting for database performance, embedding generation latency, and query response times.

**Backup & Recovery**: Implement regular database backups and test recovery procedures, particularly important given the investment in embedding generation.

**Scaling**: Consider read replicas for query workloads, connection pooling for high-concurrency scenarios, and potential sharding strategies for very large datasets.

## ü§ù Contributing

This project serves as both a learning resource and a production-ready foundation. Contributions are welcome, particularly in the areas of:

- Additional embedding model integrations
- Performance optimizations and benchmarking
- Production deployment configurations
- Advanced RAG techniques implementation

## üìö References & Further Reading

- [pgvectorscale Official Repository](https://github.com/timescale/pgvectorscale)
- [PostgreSQL as Vector Database Blog Post](https://www.timescale.com/blog/pgvector-is-now-as-fast-as-pinecone-at-75-less-cost/)
- [Timescale Vector Python Library Documentation](https://github.com/timescale/python-vector)
- [sentence-transformers Documentation](https://www.sbert.net/)
- [Anthropic Claude API Documentation](https://docs.anthropic.com/)

---

*This project demonstrates modern RAG system architecture using open-source tools and PostgreSQL, showcasing practical data science engineering skills for production AI applications.*
